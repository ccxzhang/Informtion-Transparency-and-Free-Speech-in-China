import pandas as pd
import requests
from bs4 import BeautifulSoup
import json

def request_page(url):
    '''
    '''

    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
    }

    try:
        r = requests.get(url, headers=headers)
        page= r.content
        return page

    except requests.HTTPError as e:
        print(e)
        print("HTTPError")
    except requests.RequestException as e:
        print(e)
    except:
        print("Unknown Error !")

def zhihu_scrapyer():

    file= pd.DataFrame()
    page_number= 0
    total_number= 305

    while(page_number < total_number):

        url= "https://www.zhihu.com/api/v4/questions/441839927/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_recognized%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=5&offset="+str(page_number)+"&platform=desktop&sort_by=default"

        page= request_page(url)

        text= json.loads(page)['data']

        answers= list()

        for i in text:
            answer= list()

            answer.append(i['author']['name'])  ## Get the user name
            answer.append(i['author']['gender']) ##  Get the user gender
            answer.append(i['voteup_count'])    ## Get the voteup account
            answer.append(i['comment_count'])   ## Get the comment count
            answer.append(i["url"])  ## Get the answer url
            answer.append(i['content']) ## Get the answer content

            answers.append(answer)

            file= file.append(pd.DataFrame(answers), ignore_index= True)

        page_number+= 5

    file.to_csv('~/Documents/PPOL565/zhihu_answers.csv')

zhihu_scrapyer()


bno= pd.read_csv('~/Documents/PPOL565/zhihu_answers.csv').drop(columns= ["Unnamed: 0"])
bno= bno.drop_duplicates().reset_index().drop(columns= ['index'])
bno
